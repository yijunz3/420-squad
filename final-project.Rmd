---
title: 'Indian Used Cars Price Prediction - Final Project'
author: "Jiazhen Li, Yijun Zhao, Heming Huang, Hanwen Hu"
date: '12/1/2020'
output:
  html_document:
    theme: readable
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Summary Report

## Introduction

### Background

The development speed of India's automobile industry is fast in the world. Due to India's national conditions, poor families account for a large proportion of India. Therefore, because of the price advantage, many people will buy second-hand cars instead of first-hand new cars. This has also promoted the development of the Indian used car market. In India, there are as many as 750 organized used car sales outlets, and this number will continue to grow.

### Our interests
Nowadays, more and more people consider purchasing a used car instead of buying a new one, since it is more feasible and a better investment. However, there are also many frauds and unfair pricing in the market. The most important part of the transaction (buying as well as selling) is making sure the price is fair.

India has a huge market for the used car business. According to the study on the sector, the Indian used car industry possesses significant potential, which was valued at USD 24.24 billion in 2019. Therefore, this dataset should contain much useful information, and be able to help us to construct an efficient model that can predict used/secondhand car prices given some predictor variables. 

### Data File Description
Our dataset focuses on several characteristics of used cars in India. It contains 6018 observations and 13 variables, including several categorical variables, such as Transmission Types and Fuel Types. Also, it has continuous variables: Mileage, Year and Power and discrete variable: Number of Seats. For this project, the Used Car Price would be the numeric response variable and the left being dependent variables. 

### Data File Link
The data file can be retrieved from https://www.kaggle.com/avikasliwal/used-cars-price-prediction?select=train-data.csv as "train-data.csv."


## Methods

### Original Data Profile
For further model development, it is essential to take an overview on the original data file. 

There are 6018 observations and 13 variables:

1 numeric response variable:

- `Price`: used car prices in INR Lakhs

6 categorical variables:

- `Name`: the car model names
- `Location`: the location of the car sold
- `Fuel_Type`: `Diesel`, `Petrol`, `CNG`
- `Transmission`: `Manual`, `Automatic`
- `Owner_Type`: `First`, `Second`, `Third`, `Fourth & Above`
- `Seats`: `5`, `7` and some other number of seats

6 numeric variables:

- `Year`: car edition year from 1998 to 2019
- `Kilometers_Driven`: the kilometers already driven of the car in km.
- `Mileage`:  car mileage in kmpl or km/kg
- `Engine`:  the engine volume in cc
- `Power`: the car's power in bhp
- `New Price`: the car price when it's new in INR Lakhs

The following displays the summary of the original data file and plots of repsonses variables with the numeric variables, excluding `New_Price` because of  too many missing values.
```{r}
original = read.csv("used_car.csv")
summary(original)
```

```{r, echo = FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

### Additional Data Preparation
For the original data to be used, it should be cleaned.

Data cleaning starts with the original csv file, including remove all the units behind the values and change all 0 values to null for convenience. The `Location` and `Name` are removed because they contain miscellaneous values that cannot be categorized. `New_Price` is removed because there are too few values to be used. It is also necessary to remove some values for other variables, such as resolve the units differences for Mileage. After the above alterations, it forms `used_car_cleaned.csv`. 

The next step is to clean rows which contain null values in its data.
```{r}
used_car = read.csv("used_car_cleaned.csv")
ucar = na.omit(used_car)
str(ucar)
```
Then the data contains 10 variables and 4981 entries.

Finally, it is needed to make factor variables.
```{r}
ucar$Fuel_Type = as.factor(ucar$Fuel_Type)
ucar$Transmission = as.factor(ucar$Transmission)
ucar$Owner_Type = as.factor(ucar$Owner_Type)
ucar$Seats = as.factor(ucar$Seats)
```

We want to better understand what our dataset looks like before start building models.

First we notice that our response variable Price has a right skew. A log transform can be used to adjust and normalize the distribution. 

```{r}
par(mfrow=c(1,2))
hist(ucar$Price,main = "Price", xlab = 'Price',col = 'dodgerblue')
hist(log(ucar$Price), main = "log(Price)", xlab = 'log(Price)', col = 'orange')

```

We also check the distributions of the numeric predictors and note that many of them become more normal after applying a log transformation.

```{r}
#extract the predictor names
predictor = setdiff(colnames(ucar), c("Price"))

numeric_var_count = length(which(sapply(ucar, is.numeric)))
par(mfrow=c(ceiling(numeric_var_count*2 / 6),6), mar=c(1,2,1,2))
for (p in predictor) {
  if (is.numeric(ucar[[p]])) {
    hist(ucar[[p]], main = p, xaxt='n', yaxt='n', col = 'dodgerblue')
    hist(log(ucar[[p]]), main = paste0("log(",p,")"), xaxt='n', yaxt='n', col = 'orange')
  }
}
```



### Model establishment 
The modeling process can start with a full additive model with backward aic for a good model determination.
```{r}
car_add = lm(Price ~., data = ucar)
add_back_aic = step(car_add, direction = "backward", trace = 0)
coef(add_back_aic)
```
Then, it is always important to check for model assumptions
```{r}
library(lmtest)
bptest(add_back_aic)
shapiro.test(resid(add_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated.

We can try to fix the violation of the normality assumption. 
```{r}
cook = cooks.distance(add_back_aic)
add_back_aic2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))
```

```{r, echo = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```

```{r}
shapiro.test(resid(add_back_aic2))
bptest(add_back_aic2)
```
As it can be seen from the above two Q-Q plots. Before removing the influential points, the Q-Q plot suggests severe violation of normality assumption, as the many points are far off the line. After removing the influential points, the Shapiro-Wilk test still suggests a violation of normality, and Breusch-Pagan test also still shows a violation of constant variance.

<br />

The next step is to develop a full first order and second order polynomial model with backward aic for a good model determination and check for the model assumptions.
```{r}
car_poly = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar)
poly_back_aic = step(car_poly, direction = "backward", trace = 0)
coef(poly_back_aic)
bptest(poly_back_aic)
shapiro.test(resid(poly_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(poly_back_aic)
poly_back_aic2 = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(poly_back_aic2))
bptest(poly_back_aic2)
```

<br />

In addition to above models, an full interaction model with backward aic for a good model determination can be developed, and model assumptions should be checked
```{r}
car_int = lm(Price ~.^2, data = ucar)
int_back_aic = step(car_int, direction = "backward", trace = 0)
coef(int_back_aic)
bptest(int_back_aic)
shapiro.test(resid(int_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(int_back_aic)
int_back_aic2 = lm(Price ~.^2, data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(int_back_aic2))
bptest(int_back_aic2)
```

<br />

Finally, we integrate all variable selected from the above process into one integrated model, and repeat the above process for model assumption violations check and try to fix the violations by removing influential points if there is any.

```{r}
integ_model = lm(Price ~.^2 - Engine + I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) - Year: Engine - Year:Owner_Type - Kilometers_Driven:Owner_Type - Kilometers_Driven:Power - Fuel_Type:Transmission - Fuel_Type:Owner_Type - Fuel_Type:Mileage - Transmission:Owner_Type - Transmission:Engine - Owner_Type:Mileage - Owner_Type:Engine - Owner_Type:Power - Owner_Type:Seats - Mileage:Engine , data = ucar)
summary(integ_model)
bptest(integ_model)
shapiro.test(resid(integ_model))

cook = cooks.distance(integ_model)
integ_model2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))
bptest(integ_model2)
shapiro.test(resid(integ_model2))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown above.

### Transformation

We have created different models, but many of them fails the Shapiro-Wilks and Breusch-Pagan tests. This suggests we might be able to achieve better results by applying some transformations to our the response and/or predictors.

Based on the graphs in the *Additional Data Preparation* section above, it appears that our model would benefit from log transforming the response variable `Price`.

```{r}
log_car = lm(log(Price) ~ ., data = ucar)
log_car_aic = step(log_car, direction = "backward", trace = 0)
```

```{r}
bptest(log_car_aic)
shapiro.test(resid(log_car_aic))
qqnorm(resid(log_car_aic), main = "Normal Q-Q Plot After log transforming", col = "darkgrey")
qqline(resid(log_car_aic), col = "dodgerblue", lwd = 2)
```

Although model assumptions are still violated, we can observe from the Q-Q plot that log transforming improves the normality. Next, we’ll try adding some polynomial transformations to the predictors, but also keep the log transform of the response:

```{r}
log_poly_car = lm(log(Price) ~ . +I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2) , data = ucar)
log_poly_aic = step(log_poly_car, direction = "backward", trace = 0)
```

```{r}
bptest(log_poly_aic)
shapiro.test(resid(log_poly_aic))
```

Since the model assumptions are still violated, we could continue log transforming some of the numeric predictors, but it will make the model more complicated and difficult to interpret. So we decide to stop here.

### Model Selection
We select the "best" model by comparing the LOOCV RMSE value, the adjusted $R^2$ value and the number of predictors each model contains.

A model of good performance in both prediction and explanation should have a relatively low LOOCV RMSE value, a relatively high adjusted $R^2$ value, and a relatively small number of predictors.

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

evaluate_model = function(name, model) {
  data.frame(Model = name,
             Predictor = length(coef(model)),
             LOOCV.RMSE = calc_loocv_rmse(model) , 
             Adj.R2 = summary(model)$adj.r.squared
             )
}
```

```{r}
knitr::kable(evaluate_model("add_back_aic",add_back_aic))
knitr::kable(evaluate_model("add_back_aic2",add_back_aic2))
knitr::kable(evaluate_model("poly_back_aic2",poly_back_aic2))
knitr::kable(evaluate_model("poly_back_aic",add_back_aic))
knitr::kable(evaluate_model("int_back_aic2",int_back_aic2))
knitr::kable(evaluate_model("int_back_aic",add_back_aic))
knitr::kable(evaluate_model("integ_model",integ_model))
knitr::kable(evaluate_model("integ_model2",integ_model2))
knitr::kable(evaluate_model("log_car_aic",log_car_aic))
knitr::kable(evaluate_model("log_poly-aic",log_poly_aic))
```

We see that the model `log_car_aic` is a good model for prediction, since it achieves the smallest LOOCV RMSE, but may not be the best model for explanation.

To explain a relationship, we are interested in keeping models as small as possible, since smaller models are easy to interpret.

Therefore, we would like to select the model `log_car_aic` to be the final model, since it achieves the smallest LOOCV RMSE and a relatively large adjusted $R^2$ value with 15 predictors, which is not a very big number.


## Results

The primary object of this project is to use Indian's used car data to predict used/secondhand car prices given some predictor variables. 

The original dataset contains 6018 observations and 13 variables. We first cleaned our dataset to remove unnecessary units and some predictors with insufficient or miscellaneous values, and then cleaned rows which contain null values. 

```{r}
##Summary of our final dataset
summary(ucar)
```

```{r}
##See some of the significant predictors' mean value in the dataset 
mean(ucar$Year)
mean(ucar$Mileage)
mean(ucar$Kilometers_Driven)
```

- We start our modeling process with a full additive model with backward aic for a good model determination. We checked the model assumption and found  constant variance and normality of errors are violated. We tried to fix the violation of the normality assumption, but it did not work according to test results. Next we developed a full first order and second order polynomial model with backward aic for a good model determination. We checked the model assumption of constant variance and normality of errors and found they are violated. Also, removing influential points does not fix the violations either just like the previous one. Finally, we integrated all variable selected from the above process into one integrated model, and repeated the above process for model assumption violations check and tried to fix the violations by removing influential points if there is any. However, although we have done log transformations on the model, which somehow improves our normality assumption, there are still violations existing.

- Then we started model selection by comparing the LOOCV RMSE of models. We are interested in the model which can be used to predict, but is also small enough to interpret. Hence, our final model is "log_car_aic". 


```{r,warning=FALSE,message=FALSE}
## For our determined final model, lets see the predictors we have.
length(coef(log_car_aic))
names(coef(log_car_aic))
```

```{r}
##Summary of our final chosen model 
summary(log_car_aic)
```

Check how this model performs. 

```{r,warning=FALSE,message=FALSE}
par(mfrow = c(2, 2))
plot(log_car_aic, col = "darkorange", lwd = 2)

# Evaluation
summary(log_car_aic)$adj.r.squared 
calc_loocv_rmse(log_car_aic)

#AIC
extractAIC(log_car_aic)

#BIC
extractAIC(log_car_aic,k=log(nrow(ucar)))

```


```{r}
##Using this model, let's predict the car price based on Year of 2015, Car Mileage of 20, Kilometers_Driven of 35000, Petrol Fuel Type, Manual Transmission, First Owner_Type, Engine of 1000, Power of 1000, and Seats of 5
exp(predict(log_car_aic, newdata = data.frame(Year = 2015, Mileage = 20, Kilometers_Driven = 35000, Fuel_Type = "Petrol", Transmission = "Manual", Owner_Type = "First", Engine = 1000, Power = 100, Seats = "5")))
```

## Discussion
For the 'log_add_aic', we observe that 'Mileage' would be the most important predictor among some other significant variables.
```{r}
log_car = lm(log(Price) ~ ., data = ucar)
log_car_aic = step(log_car, direction = "backward", trace = 0)
coef(summary(log_car_aic))['Mileage',]
coef(summary(log_car_aic))['Power',]
coef(summary(log_car_aic))['Engine',]
```

Significant Predictors
```{r}
sum(summary(log_car_aic)$coefficients[ ,4] < 0.05)/length(coef(log_car_aic))
summary(log_car_aic)$r.squared
```
This means 86.67% of the predictors are significant tested by the t-test at $a = 0.05$ level. Also, the model has a good multiple R-Square of 0.871, meaning 87.1% of changes in used car's price data is explained by the model.

Now, let's take a look at the variables again.
```{r}
summary(log_car_aic)$coefficients
```
At $a = 0.05$ level, Kilometers_Driven is not significant is a surprise to us, and the reason may be that these cars are usually durable for a long time.

Fuel_Type, Transmission, and most Owner_Type are significant, and it is reasonable because they are related to the cost after purchase, the convenience of operations, and resale value respectively. We can also see people in India prefer Diesel, automation, and first-hand cars.

Mileage, Engine volume, and power are major factors affecting a car's performance, so it is sensible that they are also significant. The more mileage a car has run, the lower value it is predicted to have. Engine volume and power have a positive relationship with the prices.

The number of Seat influences the usage of the car, the more seats it has, the more people it can carry, then the significance of Seats is reasonable.

The final issue of the log_car_aic model is that the violations of assumptions cannot be fixed no matter what we have done. On the one hand, a guess of the reason may be that the original data we use has some undetected problems. On the other hand, removing too much data will result in the model losing enough explanatory power. Thus, we decided not to go further in fixing the violations of the assumptions. However, we do see the assumptions get better than before both after removing influential points and transformation, which certainly makes this model stronger in explaining the subject.

## Appendix
The following code is for `Price` vs numeric variables plots of the original data file.
```{r,eval=FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

The following code is for Q-Q plots
```{r, eval = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```

The contributors of this Stat 420 Final Project are Jiazhen Li, Yijun Zhao, Heming Huang, Hanwen Hu.
