---
title: 'Indian Used Cars Price Prediction - Final Project'
date: '12/1/2020'
output:
  html_document:
    theme: readable
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Summary Report

## Introduction

### Background

The development speed of India's automobile industry is fast in the world. Due to India's national conditions, poor families account for a large proportion of India. Therefore, because of the price advantage, many people will buy second-hand cars instead of first-hand new cars. This has also promoted the development of the Indian used car market. In India, there are as many as 750 organized used car sales outlets, and this number will continue to grow.

### Our interests
Nowadays, more and more people consider purchasing a used car instead of buying a new one, since it is more feasible and a better investment. However, there are also many frauds and unfair pricing in the market. The most important part of the transaction (buying as well as selling) is making sure the price is fair.

India has a huge market for the used car business. According to the study on the sector, the Indian used car industry possesses significant potential, which was valued at USD 24.24 billion in 2019. Therefore, this dataset should contain much useful information, and be able to help us to construct an efficient model that can predict used/secondhand car prices given some predictor variables. 

### Data File Description
Our dataset focuses on several characteristics of used cars in India. It contains 6018 observations and 13 variables, including several categorical variables, such as Transmission Types and Fuel Types. Also, it has continuous variables: Mileage, Year and Power and discrete variable: Number of Seats. For this project, the Used Car Price would be the numeric response variable and the left being dependent variables. 

### Data File Link
The data file can retrived from https://www.kaggle.com/avikasliwal/used-cars-price-prediction?select=train-data.csv as "train-data.csv."


## Methods

### Original Data Profile
For further model development, it is essential to take an overview on the original data file. 

There are 6018 observations and 13 variables:

1 numeric response variable:

- `Price`: used car prices in INR Lakhs

6 categorical variables:

- `Name`: the car model names
- `Location`: the location of the car sold
- `Fuel_Type`: `Diesel`, `Petrol`, `CNG`
- `Transmission`: `Manual`, `Automatic`
- `Owner_Type`: `First`, `Second`, `Third`, `Fourth & Above`
- `Seats`: `5`, `7` and some other number of seats

6 numeric variables:

- `Year`: car edition year from 1998 to 2019
- `Kilometers_Driven`: the kilometers already driven of the car in km.
- `Mileage`:  car mileage in kmpl or km/kg
- `Engine`:  the engine volume in cc
- `Power`: the car's power in bhp
- `New Price`: the car price when it's new in INR Lakhs

The following displays the summary of the original data file and plots of repsonses variables with the numeric variables, excluding `New_Price` because of  too many missing values.
```{r}
original = read.csv("used_car.csv")
summary(original)
```
```{r, echo = FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

### Additional Data Preparation
For the original data to be used, it should be cleaned.

Data cleaning starts with the original csv file, including remove all the units behind the values and change all 0 values to null for convenience. The `Location` and `Name` are removed because they contain miscellaneous values that cannot be categorized. `New_Price` is removed because there are too few values to be used. It is also necessary to remove some values for other variables, such as resolve the units differences for Mileage. After the above alterations, it forms `used_car_cleaned.csv`. 

The next step is to clean rows which contain null values.
```{r}
used_car = read.csv("used_car_cleaned.csv")
ucar = na.omit(used_car)
str(ucar)
```
Then the data contains 10 variables and 4981 entries.

Finally, it is needed to make factor variables.
```{r}
ucar$Fuel_Type = as.factor(ucar$Fuel_Type)
ucar$Transmission = as.factor(ucar$Transmission)
ucar$Owner_Type = as.factor(ucar$Owner_Type)
ucar$Seats = as.factor(ucar$Seats)
```

### Model establishment 
The modeling process can start with a full additive model with backward AIC for a good model determination.
```{r}
car_add = lm(Price ~., data = ucar)
add_back_aic = step(car_add, direction = "backward", trace = 0)
coef(add_back_aic)
```
Then, it is always important to check for model assumptions
```{r}
library(lmtest)
bptest(add_back_aic)
shapiro.test(resid(add_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated.

We can try to fix the violation of the normality assumption. 
```{r, echo = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

cook = cooks.distance(add_back_aic)
add_back_aic2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```

```{r}
shapiro.test(resid(add_back_aic2))
bptest(add_back_aic2)
```
As it can be seen from the above two Q-Q plots. Before removing the influential points, the Q-Q plot suggests severe violation of normality assumption, as the many points are far off the line. After removing the influential points, the Q-Q plot is slightly better, however the Shapiro-Wilk test still suggests a violation of normality, and Breusch-Pagan test also still shows a violation of constant variance.

<br />

The next step is to develop a full first order and second order polynomial model with backward aic for a good model determination and check for the model assumptions.
```{r}
car_poly = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar)
poly_back_aic = step(car_poly, direction = "backward", trace = 0)
coef(poly_back_aic)
bptest(poly_back_aic)
shapiro.test(resid(poly_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(poly_back_aic)
poly_back_aic2 = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(poly_back_aic2))
bptest(poly_back_aic2)
```

<br />

In addition to above models, a full interaction model with backward aic for a good model determination can be developed, and model assumptions should be checked.
```{r}
car_int = lm(Price ~.^2, data = ucar)
int_back_aic = step(car_int, direction = "backward", trace = 0)
coef(int_back_aic)
bptest(int_back_aic)
shapiro.test(resid(int_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(int_back_aic)
int_back_aic2 = lm(Price ~.^2, data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(int_back_aic2))
bptest(int_back_aic2)
```

<br />

Finally, we integrate all variable selected from the above process into one integrated model, and repeat the above process for model assumption violations check and try to fix the violations by removing influential points if there is any.

```{r}
integ_model = lm(Price ~.^2 - Engine + I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) - Year: Engine - Year:Owner_Type - Kilometers_Driven:Owner_Type - Kilometers_Driven:Power - Fuel_Type:Transmission - Fuel_Type:Owner_Type - Fuel_Type:Mileage - Transmission:Owner_Type - Transmission:Engine - Owner_Type:Mileage - Owner_Type:Engine - Owner_Type:Power - Owner_Type:Seats - Mileage:Engine , data = ucar)
summary(integ_model)
bptest(integ_model)
shapiro.test(resid(integ_model))

cook = cooks.distance(integ_model)
integ_model2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))
bptest(integ_model2)
shapiro.test(resid(integ_model2))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown above.

### Model Selection
To begin with, we can use information from the summary() function to compare Adjusted $R_2$ values of the models. 

```{r}
summary(car_add)$adj.r.squared
summary(add_back_aic)$adj.r.squared
summary(add_back_aic2)$adj.r.squared
summary(car_poly)$adj.r.squared
summary(poly_back_aic)$adj.r.squared
summary(poly_back_aic2)$adj.r.squared
summary(car_int)$adj.r.squared
summary(int_back_aic)$adj.r.squared
summary(int_back_aic2)$adj.r.squared
summary(integ_model)$adj.r.squared
summary(integ_model2)$adj.r.squared

```
Note that the modified interaction model "int_back_aic2" is preferred since it has the greatest adjusted $R_2$ value.

We can also calculate the LOOCV RMSE for each,
```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

```{r}
calc_loocv_rmse(car_add)
calc_loocv_rmse(car_int)
calc_loocv_rmse(car_poly)
calc_loocv_rmse(add_back_aic2)
calc_loocv_rmse(poly_back_aic2)
calc_loocv_rmse(int_back_aic2)
calc_loocv_rmse(add_back_aic)
calc_loocv_rmse(poly_back_aic)
calc_loocv_rmse(int_back_aic)
calc_loocv_rmse(integ_model)
calc_loocv_rmse(integ_model2)

```
We see that the modified polynomial model is the good model for prediction, since it achieves the smallest LOOCV RMSE, but may not be the best model for explanation.

Since the modified interaction model and the modified polynomial model are both of good performance, we are interested in keeping models as small as possible, since smaller models are easier to interpret. 

```{r}
length(coef(int_back_aic2))
length(coef(poly_back_aic2))
```

Therefore, we would like to select the model "poly_back_aic2" as our final model.


## Results

## Discussion

## Appendix
The following code is for `Price` vs numeric variables plots of the original data file.
```{r,eval=FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

The following code is for Q-Q plots
```{r, eval = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

cook = cooks.distance(add_back_aic)
add_back_aic2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```
