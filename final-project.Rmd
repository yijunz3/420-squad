---
title: 'Indian Used Cars Price Prediction - Final Project'
author: "Jiazhen Li, Yijun Zhao, Heming Huang, Hanwen Hu"
date: '12/1/2020'
output:
  html_document:
    theme: readable
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Summary Report

## Introduction

### Background

The development speed of India's automobile industry is fast in the world. Due to India's national conditions, poor families account for a large proportion of India. Therefore, because of the price advantage, many people will buy second-hand cars instead of first-hand new cars. This has also promoted the development of the Indian used car market. In India, there are as many as 750 organized used car sales outlets, and this number will continue to grow.

### Our interests
Nowadays, more and more people consider purchasing a used car instead of buying a new one, since it is more feasible and a better investment. However, there are also many frauds and unfair pricing in the market. The most important part of the transaction (buying as well as selling) is making sure the price is fair.

India has a huge market for the used car business. According to the study on the sector, the Indian used car industry possesses significant potential, which was valued at USD 24.24 billion in 2019. Therefore, this dataset should contain much useful information, and be able to help us to construct an efficient model that can predict used/secondhand car prices given some predictor variables. 

### Data File Description
Our dataset focuses on several characteristics of used cars in India. It contains 6018 observations and 13 variables, including several categorical variables, such as Transmission Types and Fuel Types. Also, it has continuous variables: Mileage, Year and Power and discrete variable: Number of Seats. For this project, the Used Car Price would be the numeric response variable and the left being dependent variables. 

### Data File Link
The data file can be retrieved from https://www.kaggle.com/avikasliwal/used-cars-price-prediction?select=train-data.csv as "train-data.csv."


## Methods

### Original Data Profile
For further model development, it is essential to take an overview on the original data file. 

There are 6018 observations and 13 variables:

1 numeric response variable:

- `Price`: used car prices in INR Lakhs

6 categorical variables:

- `Name`: the car model names
- `Location`: the location of the car sold
- `Fuel_Type`: `Diesel`, `Petrol`, `CNG`
- `Transmission`: `Manual`, `Automatic`
- `Owner_Type`: `First`, `Second`, `Third`, `Fourth & Above`
- `Seats`: `5`, `7` and some other number of seats

6 numeric variables:

- `Year`: car edition year from 1998 to 2019
- `Kilometers_Driven`: the kilometers already driven of the car in km.
- `Mileage`:  car mileage in kmpl or km/kg
- `Engine`:  the engine volume in cc
- `Power`: the car's power in bhp
- `New Price`: the car price when it's new in INR Lakhs

The following displays the summary of the original data file and plots of repsonses variables with the numeric variables, excluding `New_Price` because of  too many missing values.
```{r}
original = read.csv("used_car.csv")
summary(original)
```

```{r, echo = FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

### Additional Data Preparation
For the original data to be used, it should be cleaned.

Data cleaning starts with the original csv file, including remove all the units behind the values and change all 0 values to null for convenience. The `Location` and `Name` are removed because they contain miscellaneous values that cannot be categorized. `New_Price` is removed because there are too few values to be used. It is also necessary to remove some values for other variables, such as resolve the units differences for Mileage. After the above alterations, it forms `used_car_cleaned.csv`. 

The next step is to clean rows which contain null values in its data.
```{r}
used_car = read.csv("used_car_cleaned.csv")
ucar = na.omit(used_car)
str(ucar)
```
Then the data contains 10 variables and 4981 entries.

Finally, it is needed to make factor variables.
```{r}
ucar$Fuel_Type = as.factor(ucar$Fuel_Type)
ucar$Transmission = as.factor(ucar$Transmission)
ucar$Owner_Type = as.factor(ucar$Owner_Type)
ucar$Seats = as.factor(ucar$Seats)
```

### Model establishment 
The modeling process can start with a full additive model with backward aic for a good model determination.
```{r}
car_add = lm(Price ~., data = ucar)
add_back_aic = step(car_add, direction = "backward", trace = 0)
coef(add_back_aic)
```
Then, it is always important to check for model assumptions
```{r}
library(lmtest)
bptest(add_back_aic)
shapiro.test(resid(add_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated.

We can try to fix the violation of the normality assumption. 
```{r}
cook = cooks.distance(add_back_aic)
add_back_aic2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))
```

```{r, echo = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```

```{r}
shapiro.test(resid(add_back_aic2))
bptest(add_back_aic2)
```
As it can be seen from the above two Q-Q plots. Before removing the influential points, the Q-Q plot suggests severe violation of normality assumption, as the many points are far off the line. After removing the influential points, the Q-Q plot is slightly better, however the Shapiro-Wilk test still suggests a violation of normality, and Breusch-Pagan test also still shows a violation of constant variance.

<br />

The next step is to develop a full first order and second order polynomial model with backward aic for a good model determination and check for the model assumptions.
```{r}
car_poly = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar)
poly_back_aic = step(car_poly, direction = "backward", trace = 0)
coef(poly_back_aic)
bptest(poly_back_aic)
shapiro.test(resid(poly_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(poly_back_aic)
poly_back_aic2 = lm(Price ~.+ I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) +I(Power^2), data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(poly_back_aic2))
bptest(poly_back_aic2)
```

<br />

In addition to above models, an full interaction model with backward aic for a good model determination can be developed, and model assumptions should be checked
```{r}
car_int = lm(Price ~.^2, data = ucar)
int_back_aic = step(car_int, direction = "backward", trace = 0)
coef(int_back_aic)
bptest(int_back_aic)
shapiro.test(resid(int_back_aic))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown below.

```{r}
cook = cooks.distance(int_back_aic)
int_back_aic2 = lm(Price ~.^2, data = ucar, subset = (cook < 4 / length(cook)))
shapiro.test(resid(int_back_aic2))
bptest(int_back_aic2)
```

<br />

Finally, we integrate all variable selected from the above process into one integrated model, and repeat the above process for model assumption violations check and try to fix the violations by removing influential points if there is any.

```{r}
integ_model = lm(Price ~.^2 - Engine + I(Kilometers_Driven^2) +I(Mileage^2) + I(Engine^2) - Year: Engine - Year:Owner_Type - Kilometers_Driven:Owner_Type - Kilometers_Driven:Power - Fuel_Type:Transmission - Fuel_Type:Owner_Type - Fuel_Type:Mileage - Transmission:Owner_Type - Transmission:Engine - Owner_Type:Mileage - Owner_Type:Engine - Owner_Type:Power - Owner_Type:Seats - Mileage:Engine , data = ucar)
summary(integ_model)
bptest(integ_model)
shapiro.test(resid(integ_model))

cook = cooks.distance(integ_model)
integ_model2 = lm(Price ~., data = ucar, subset = (cook < 4 / length(cook)))
bptest(integ_model2)
shapiro.test(resid(integ_model2))
```
Since the p-value for both Breusch-Pagan test and Shapiro-Wilk test are very small, the model assumption of constant variance and normality of errors are violated. Also, removing influential points does not fix the violations either, as shown above.

### Model Selection
To begin with, we can use information from the summary() function to compare Adjusted $R_2$ values of the models. 

```{r}
summary(car_add)$adj.r.squared
summary(add_back_aic)$adj.r.squared
summary(add_back_aic2)$adj.r.squared
summary(car_poly)$adj.r.squared
summary(poly_back_aic)$adj.r.squared
summary(poly_back_aic2)$adj.r.squared
summary(car_int)$adj.r.squared
summary(int_back_aic)$adj.r.squared
summary(int_back_aic2)$adj.r.squared
summary(integ_model)$adj.r.squared
summary(integ_model2)$adj.r.squared
```
Note that the modified interaction model "int_back_aic2" is preferred since it has the greatest adjusted $R_2$ value.

We can also calculate the LOOCV RMSE for each,
```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

```{r}
calc_loocv_rmse(car_add)
calc_loocv_rmse(car_int)
calc_loocv_rmse(car_poly)
calc_loocv_rmse(add_back_aic2)
calc_loocv_rmse(poly_back_aic2)
calc_loocv_rmse(int_back_aic2)
calc_loocv_rmse(add_back_aic)
calc_loocv_rmse(poly_back_aic)
calc_loocv_rmse(int_back_aic)
calc_loocv_rmse(integ_model)
calc_loocv_rmse(integ_model2)
```
We see that the modified polynomial model is a good model for prediction, since it achieves the smallest LOOCV RMSE, but may not be the best model for explanation.

Since the modified interaction model and the modified polynomial model are both of good performance, we are interested in keeping models as small as possible, since smaller models are easier to interpret. 

```{r}
length(coef(int_back_aic2))
length(coef(poly_back_aic2))
```

Therefore, we would like to select the model "poly_back_aic2" as our final model.


## Results

The primary object of this project is to use Indian's used car data to predict used/secondhand car prices given some predictor variables. 

The original dataset contains 6018 observations and 13 variables. We first cleaned our dataset to remove unnecessary units and some predictors with insufficient or miscellaneous values, and then cleaned rows which contain null values. 

```{r}
##Summary of our final dataset
summary(ucar)
```

```{r}
##See some of the significant predictors' mean value in the dataset 
mean(ucar$Year)
mean(ucar$Mileage)
mean(ucar$Kilometers_Driven)
```

- We start our modeling process with a full additive model with backward aic for a good model determination. We checked the model assumption and found  constant variance and normality of errors are violated. We tried to fix the violation of the normality assumption, but it did not work according to test results. Next we developed a full first order and second order polynomial model with backward aic for a good model determination. We checked the model assumption of constant variance and normality of errors and found they are violated. Also, removing influential points does not fix the violations either just like the previous one. Finally, we integrated all variable selected from the above process into one integrated model, and repeated the above process for model assumption violations check and tried to fix the violations by removing influential points if there is any. We still found the model assumption of constant variance and normality of errors are violated.

- Then we started model selection with comparing adjusted $R_2$ values and found the modified interaction model "int_back_aic2" is preferred due to the greatest adjusted $R_2$ value. We also calculated the LOOCV RMSE for each and found the modified polynomial model is a good model for prediction for its LOOCV RMSE. These two models both perform well, but we are interested in keeping models as small as possible, since smaller models are easier to interpret. So, our final model is "poly_back_aic2". 


```{r,warning=FALSE,message=FALSE}
## For our determined final model, lets see the predictors we have.
length(coef(poly_back_aic2))
names(coef(poly_back_aic2))
```

```{r}
##Summary of our final chosen model 
summary(poly_back_aic2)
```

Check how this model performs. 

```{r,warning=FALSE,message=FALSE}
par(mfrow = c(2, 2))
plot(poly_back_aic2, col = "darkorange", lwd = 2)

# Evaluation
summary(poly_back_aic2)$adj.r.squared 
calc_loocv_rmse(poly_back_aic2)

#AIC
extractAIC(poly_back_aic2)

#BIC
extractAIC(poly_back_aic2,k=log(nrow(ucar)))

```


```{r}
##Using this model, let's predict the car price based on Year of 2015, Car Mileage of 20, Kilometers_Driven of 35000, Petrol Fuel Type, Manual Transmission, First Owner_Type, Engine of 1000, Power of 1000, and Seats of 5
predict(poly_back_aic2, newdata = data.frame(Year = 2015, Mileage = 20, Kilometers_Driven = 35000, Fuel_Type = "Petrol", Transmission = "Manual", Owner_Type = "First", Engine = 1000, Power = 100, Seats = "5"))
```



## Discussion
For the 'integ_model', we observe that 'Mileage' would be the most important predictor.
```{r}
integ_aic = step(integ_model, trace = 0)
coef(summary(integ_aic))['Mileage',]
```

Significant Predictors
```{r}
sum(summary(integ_aic)$coefficients[ ,4] < 0.05)/length(coef(integ_aic))
summary(integ_aic)$coefficients[summary(integ_aic)$coefficients[ ,4] < 0.05,]
summary(integ_aic)$r.squared
```

```{r}
analysis = anova(integ_aic, integ_model, test = "F")
analysis$`Pr(>F)`[2]
```

As we known, the result is larger than $a = 0.05$. Therefore, we cannot reject the null!

## Appendix
The follwoing code is for `Price` vs numeric variables plots of the original data file.
```{r,eval=FALSE}
plot(Price ~ Kilometers_Driven, data = original, pch = 20, col = "darkgreen",main = "Price vs Kilometers_Driven", cex = 1)
plot(Price ~ Power, data = original, pch = 20, col = "darkgreen", main = "Price vs Power", cex = 1)
plot(Price ~ Mileage, data = original, pch = 20, col = "darkgreen", main = "Price vs Mileage", cex = 1)
plot(Price ~ Engine, data = original, pch = 20, col = "darkgreen", main = "Price vs Engine", cex = 1)
plot(Price ~ Year, data = original, pch = 20, col = "darkgreen", main = "Price vs Year", cex = 1)
```

The following code is for Q-Q plots
```{r, eval = FALSE}
qqnorm(resid(add_back_aic), main = "Normal Q-Q Plot Before Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic), col = "dodgerblue", lwd = 2)

qqnorm(resid(add_back_aic2), main = "Normal Q-Q Plot After Removing Influential Points", col = "darkgrey")
qqline(resid(add_back_aic2), col = "dodgerblue", lwd = 2)
```

The contributors of this Stat 420 Final Project are Jiazhen Li, Yijun Zhao, Heming Huang, Hanwen Hu.
